{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an RAG chatbot\n",
    "\n",
    "By [Purinat Pattanakeaw](<https://www.github.com/Purinat33>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Preprocessing\n",
    "\n",
    "* Remove misc. contents like document's title in footer of every page using tools like `fitz`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Reading data\n",
    "\n",
    "Using `SimpleDirectoryReader` which reads an entire directory, including files like images, PDFs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/understanding/loading/loading/\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# We will be reading it in the storage steps\n",
    "# documents = SimpleDirectoryReader(\"../docs\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model\n",
    "\n",
    "Using `sentence-transformers` Embedding Model via HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Caching model\n",
    "# https://python.langchain.com/v0.2/docs/how_to/caching_embeddings/\n",
    "# https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceEmbeddings.html\n",
    "store = LocalFileStore('cache')\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    hf, store, namespace=model_name\n",
    ")\n",
    "\n",
    "Settings.embed_model = LangchainEmbedding(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/understanding/loading/loading/\n",
    "# https://medium.com/@kofsitho/basic-tutorial-rag-with-llama-index-8927a5716dd1\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "Settings.splitter = SentenceSplitter(chunk_size=512, chunk_overlap=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Storage and Persistent Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage exists at ./persist/: Loading\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "import os\n",
    "\n",
    "# We moved the loading logic of loading documents here so we can check for persistent\n",
    "# Check if persistent storage exists, if so: load from there.\n",
    "persist_dir = './persist/'\n",
    "if os.path.exists(persist_dir) and len(os.listdir(persist_dir)) > 0:\n",
    "    print(f\"Storage exists at {persist_dir}: Loading\")\n",
    "    storage_context = StorageContext.from_defaults(persist_dir='./persist/')\n",
    "    index = load_index_from_storage(storage_context=storage_context)\n",
    "else:   \n",
    "    print(f\"{persist_dir} not exists: Performing loading\")\n",
    "    documents = SimpleDirectoryReader(\"./docs/\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set defeault retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# https://medium.com/@kofsitho/basic-tutorial-rag-with-llama-index-8927a5716dd1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m base_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever(similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m source_nodes \u001b[38;5;241m=\u001b[39m base_retriever\u001b[38;5;241m.\u001b[39mretrieve(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetabolism\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_nodes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@kofsitho/basic-tutorial-rag-with-llama-index-8927a5716dd1\n",
    "base_retriever = index.as_retriever(similarity_top_k=3)\n",
    "source_nodes = base_retriever.retrieve(\"metabolism\")\n",
    "print(f\"Score: {source_nodes[0].score:.3f}\")\n",
    "print(f\"Content:\\n {source_nodes[0].get_content()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
